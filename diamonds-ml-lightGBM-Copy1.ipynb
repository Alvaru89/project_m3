{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/diamonds_train.csv')\n",
    "test = pd.read_csv('data/diamonds_test.csv')\n",
    "sample_sub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_dict={'Ideal':5,\n",
    "'Premium':4,\n",
    "'Very Good':3,\n",
    "'Good':2,\n",
    "'Fair':1}\n",
    "\n",
    "clarity_dict={'IF':8,\n",
    "'VVS1':7,\n",
    "'VVS2':6,\n",
    "'VS1':5,\n",
    "'VS2':4,\n",
    "'SI1':3,\n",
    "'SI2':2,\n",
    "'I1':1}\n",
    "\n",
    "color_dict={'D':7,\n",
    "'E':6,\n",
    "'F':5,\n",
    "'G':4,\n",
    "'H':3,\n",
    "'I':2,\n",
    "'J':1}\n",
    "\n",
    "train['num_cut']=train.cut.apply(lambda x: cut_dict[x])\n",
    "train['num_clarity']=train.clarity.apply(lambda x: clarity_dict[x])\n",
    "train['num_color']=train.color.apply(lambda x: color_dict[x])\n",
    "train['cut/carat']=train.num_cut/train.carat\n",
    "train['clarity/carat']=train.num_clarity/train.carat\n",
    "train['color/carat']=train.num_color/train.carat\n",
    "train['depth/carat']=train.depth/train.carat\n",
    "train['table/carat']=train.table/train.carat\n",
    "\n",
    "test['num_cut']=test.cut.apply(lambda x: cut_dict[x])\n",
    "test['num_clarity']=test.clarity.apply(lambda x: clarity_dict[x])\n",
    "test['num_color']=test.color.apply(lambda x: color_dict[x])\n",
    "test['cut/carat']=test.num_cut/test.carat\n",
    "test['clarity/carat']=test.num_clarity/test.carat\n",
    "test['color/carat']=test.num_color/test.carat\n",
    "test['depth/carat']=test.depth/test.carat\n",
    "test['table/carat']=test.table/test.carat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filler_xyz(carat,depth,x,y,z):\n",
    "    if x==y==z==0:\n",
    "        x=(carat*100/(0.006*depth))**(1/3)\n",
    "        y=x\n",
    "        z=depth*x/100\n",
    "    if x==z==0:\n",
    "        x=y\n",
    "        z=depth*y/100\n",
    "    if z==0:\n",
    "        z=depth*(x+y)/200\n",
    "    return x,y,z\n",
    "\n",
    "    \n",
    "train['n'] = train.apply(lambda x: filler_xyz(x['carat'],x['depth'],x['x'],x['y'],x['z']), axis=1)\n",
    "train['x'] = train.apply(lambda x: x['n'][0], axis=1)\n",
    "train['y'] = train.apply(lambda x: x['n'][1], axis=1)\n",
    "train['z'] = train.apply(lambda x: x['n'][2], axis=1)\n",
    "\n",
    "train.drop('n',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['n'] = test.apply(lambda x: filler_xyz(x['carat'],x['depth'],x['x'],x['y'],x['z']), axis=1)\n",
    "test['x'] = test.apply(lambda x: x['n'][0], axis=1)\n",
    "test['y'] = test.apply(lambda x: x['n'][1], axis=1)\n",
    "test['z'] = test.apply(lambda x: x['n'][2], axis=1)\n",
    "\n",
    "test.drop('n',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "\n",
    "train['circular']=(train['x']+train['y'])/2\n",
    "test['circular']=(test['x']+test['y'])/2\n",
    "\n",
    "train['L/W']=train['x']/train['y'] \n",
    "train['L/W'].fillna(0,inplace=True)\n",
    "\n",
    "test['L/W']=test['x']/test['y']\n",
    "test['L/W'].fillna(0,inplace=True)\n",
    "\n",
    "train['density']=train['carat']/(train['x']*train['y']*train['z'])\n",
    "test['density']=test['carat']/(test['x']*test['y']*test['z'])\n",
    "\n",
    "cat_features = ['cut', 'color', 'clarity']\n",
    "num_features = ['carat', 'depth', 'table', 'x', 'y', 'z','L/W','circular','density',\n",
    "               'num_cut', 'num_clarity', 'num_color', 'cut/carat', 'clarity/carat', 'color/carat',\n",
    "               'depth/carat','table/carat']\n",
    "\n",
    "for cat_feat in cat_features:\n",
    "    train[cat_feat] = train[cat_feat].astype('category')\n",
    "    test[cat_feat] = test[cat_feat].astype('category')\n",
    "    \n",
    "# cat_df = pd.get_dummies(train[cat_features])\n",
    "# num_df = train.loc[:,num_features]\n",
    "# train_df = pd.concat([cat_df, num_df], axis=1)\n",
    "\n",
    "# cat_df = pd.get_dummies(test[cat_features])\n",
    "# num_df = test.loc[:,num_features]\n",
    "# test_df = pd.concat([cat_df, num_df], axis=1)\n",
    "\n",
    "\n",
    "# features = list(cat_df.columns) + list(num_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['density']>0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['density'], bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.get_dummies(train[cat_features])\n",
    "num_df = train.loc[:,num_features]\n",
    "train_df = pd.concat([cat_df, num_df], axis=1)\n",
    "\n",
    "cat_df = pd.get_dummies(test[cat_features])\n",
    "num_df = test.loc[:,num_features]\n",
    "test_df = pd.concat([cat_df, num_df], axis=1)\n",
    "\n",
    "\n",
    "features = list(cat_df.columns) + list(num_df.columns)\n",
    "\n",
    "\n",
    "data_pca = train_df[[x for x in train_df.columns if x != 'price']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_rescaled = scaler.fit_transform(data_pca)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(data_rescaled)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.ylim([0,1])\n",
    "plt.title('CumSum')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(PCA(n_components=2).fit_transform(data_pca))\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataset[0], dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1=dataset[1]>8\n",
    "filt2=dataset[0]>14\n",
    "filt3=dataset[1]<-7.5\n",
    "\n",
    "dropping=dataset[filt1|filt2|filt3].index\n",
    "train.drop(dropping, axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train[14708:14709])\n",
    "display(train[21603:21604])\n",
    "\n",
    "train.drop(21603,axis=0,inplace=True)\n",
    "train.drop(14708,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = \\\n",
    "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), \n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='constant', fill_value='missing'))])\n",
    "categorical_transformer =Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = \\\n",
    "ColumnTransformer(transformers=[('num', numeric_transformer, num_features),\n",
    "                                ('cat', categorical_transformer, cat_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_transformer = Pipeline(steps=[('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=num_features+cat_features\n",
    "X=train[features]\n",
    "y=train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  39.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  49.6s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  40.5s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  38.1s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  46.2s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  42.4s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  40.3s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  43.1s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  28.8s\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[CV] END .................................................... total time=  24.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "523.1096425046878"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model= Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('scaler', final_transformer),\n",
    "                       ('regressor', LGBMRegressor(boosting='dart',\n",
    "                                                   n_estimators=1000,\n",
    "                                                   max_depth=150,\n",
    "                                                   num_leaves=80,\n",
    "                                                   n_jobs=-1))])\n",
    "# model.fit(X=X, y=y)\n",
    "# print(\"model created!\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, \n",
    "                         X, \n",
    "                         y, \n",
    "                         scoring='neg_root_mean_squared_error', \n",
    "                         cv=10, verbose=2)\n",
    "np.mean(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n"
     ]
    }
   ],
   "source": [
    "model.fit(X=X, y=y)\n",
    "y_pred = model.predict(test[features])\n",
    "if y_pred.min()<200:\n",
    "    raise ValueError(f'price min: {y_pred.min()}')\n",
    "submission_df = pd.DataFrame({'id': test['id'], 'price': y_pred})\n",
    "submission_df.head()\n",
    "submission_df.to_csv('submission_random_trial.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__learning_rate': [0.001, 0.01, 0.05, 0.1,0.2],\n",
    "}\n",
    "\n",
    "param_grid_dart = {\n",
    "    'regressor__drop_rate': [0.1, 0.5, 0.9],\n",
    "    'regressor__uniform_drop': [True,False],\n",
    "    'regressor__drop_seed': [2,4,6],\n",
    "    'regressor__max_drop': [20,50, 80],\n",
    "    'regressor__skip_drop':[0.1,0.5,0.9]\n",
    "}\n",
    "# 'gbdt','dart', 'goss'\n",
    "grid_search = GridSearchCV(model, \n",
    "                                 param_grid, \n",
    "                                 cv=10, \n",
    "                                 verbose=10, \n",
    "                                 scoring='neg_root_mean_squared_error', \n",
    "                                 n_jobs=-1\n",
    "                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "full=datetime.now()\n",
    "hour=datetime.now().hour\n",
    "minute=datetime.now().minute \n",
    "print(full)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(full)\n",
    "print(f'elapsed time: {(datetime.now()-full)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(-grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pactools.grid_search import GridSearchCVProgressBar\n",
    "\n",
    "gscv = GridSearchCVProgressBar(model, param_grid=param_grid, cv=10,\n",
    "                               return_train_score=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para retocar!!!!!!!!\n",
    "\n",
    "def plot_results(index='dar__ordar', columns='dar__ordriv'):\n",
    "    \"\"\"Select two hyperparameters from which we plot the fluctuations\"\"\"\n",
    "    index = 'param_' + index\n",
    "    columns = 'param_' + columns\n",
    "\n",
    "    # prepare the results into a pandas.DataFrame\n",
    "    df = pd.DataFrame(gscv.cv_results_)\n",
    "\n",
    "    # Remove the other by selecting their best values (from gscv.best_params_)\n",
    "    other = [c for c in df.columns if c[:6] == 'param_']\n",
    "    other.remove(index)\n",
    "    other.remove(columns)\n",
    "    for col in other:\n",
    "        df = df[df[col] == gscv.best_params_[col[6:]]]\n",
    "\n",
    "    # Create pivot tables for easy plotting\n",
    "    table_mean = df.pivot_table(index=index, columns=columns,\n",
    "                                values=['mean_test_score'])\n",
    "    table_std = df.pivot_table(index=index, columns=columns,\n",
    "                               values=['std_test_score'])\n",
    "\n",
    "    # plot the pivot tables\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    for col_mean, col_std in zip(table_mean.columns, table_std.columns):\n",
    "        table_mean[col_mean].plot(ax=ax, yerr=table_std[col_std], marker='o',\n",
    "                                  label=col_mean)\n",
    "    plt.title('Grid-search results (higher is better)')\n",
    "    plt.ylabel('log-likelihood compared to an AR(0)')\n",
    "    plt.legend(title=table_mean.columns.names)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_results(index='dar__ordar', columns='dar__ordriv')\n",
    "plot_results(index='driver__low_fq', columns='driver__low_fq_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dart features\n",
    "\n",
    "# drop_rate, default = 0.1, type = double, aliases: rate_drop, constraints: 0.0 <= drop_rate <= 1.0\n",
    "# uniform_drop, default = false, type = bool\n",
    "# drop_seed , default = 4, type = int\n",
    "# max_drop , default = 50, type = int, <=0 means no limit\n",
    "# skip_drop , default = 0.5, type = double, constraints: 0.0 <= skip_drop <= 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypteropt\n",
    "\n",
    "#https://www.scikit-yb.org/en/latest/api/model_selection/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(test[features])\n",
    "if y_pred.min()<200:\n",
    "    raise ValueError(f'price min: {y_pred.min()}')\n",
    "submission_df = pd.DataFrame({'id': test['id'], 'price': y_pred})\n",
    "submission_df.head()\n",
    "submission_df.to_csv('submission_lGBM_grid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack_env]",
   "language": "python",
   "name": "conda-env-ironhack_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
